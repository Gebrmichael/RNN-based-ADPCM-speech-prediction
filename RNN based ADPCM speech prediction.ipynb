{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a379f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @Author: Gebremichael\n",
    "# @File: dialogic_ADPCM.py\n",
    "import wave\n",
    "import numpy as np\n",
    "\n",
    "# table of index\n",
    "IndexTable = [-1, -1, -1, -1, 2, 4, 6, 8, -1, -1, -1, -1, 2, 4, 6, 8]\n",
    "\n",
    "# table of  quantizer step size\n",
    "StepSizeTable = [7, 8, 9, 10, 11, 12, 13, 14, 16, 17, 19, 21, 23, 25, 28, 31, 34, 37, 41, 45, 50, 55, 60, 66, 73, 80, 88, 97, 107, 118, 130, 143, 157, 173, 190, 209, 230, 253, 279, 307, 337, 371, 408, 449, 494, 544, 598, 658, 724, 796, 876, 963, 1060, 1166, 1282, 1411, 1552, 1707, 1878, 2066, 2272, 2499, 2749, 3024, 3327, 3660, 4026, 4428, 4871, 5358, 5894, 6484, 7132, 7845, 8630, 9493, 10442, 11487, 12635, 13899, 15289, 16818, 18500, 20350, 22385, 24623, 27086, 29794, 32767]\n",
    "\n",
    "# ADPCM_Encode.\n",
    "# sample: a 16-bit PCM sample\n",
    "# retval : a 4-bit ADPCM sample\n",
    "predsample = 0\n",
    "index = 0\n",
    "\n",
    "# This method is the ADPCM encoder\n",
    "def ADPCM_Encode(sample):\n",
    "    global index\n",
    "    global predsample\n",
    "    global diffq\n",
    "    global ore_diff\n",
    "    global error_signal\n",
    "    \n",
    "    code = 0\n",
    "    \n",
    "    step_size = StepSizeTable[index]\n",
    "\n",
    "    # compute diff and record sign and absolut value\n",
    "          \n",
    "    diff = sample - predsample\n",
    "    ore_diff=diff\n",
    "    #print('diff sample:' ,ore_diff)\n",
    "    if diff < 0:\n",
    "        code = 8\n",
    "        diff = -diff\n",
    "\n",
    "    # quantize the diff into ADPCM code\n",
    "    # inverse quantize the code into a predicted diff\n",
    "    tmpstep = step_size\n",
    "    diffq = step_size >> 3\n",
    "    if diff >= tmpstep:\n",
    "        code = code | 0x04\n",
    "        diff -= tmpstep\n",
    "        diffq = diffq + step_size\n",
    "\n",
    "    tmpstep = tmpstep >> 1\n",
    "    if diff >= tmpstep:\n",
    "        code = code | 0x02\n",
    "        diff = diff - tmpstep\n",
    "        diffq = diffq + (step_size >> 1)\n",
    "\n",
    "    tmpstep = tmpstep >> 1\n",
    "    if diff >= tmpstep:\n",
    "        code = code | 0x01\n",
    "        diffq = diffq + (step_size >> 2)\n",
    "    \n",
    "    # fixed predictor to get new predicted sample\n",
    "    #predsample=predictor(code,predsample,diffq)\n",
    "    \n",
    "    #print(\"predsample\",predsample)\n",
    "    # find new stepsize index\n",
    "    index += IndexTable[code]\n",
    "\n",
    "    # check for overflow\n",
    "    if index < 0:\n",
    "        index = 0\n",
    "\n",
    "    if index > 88:\n",
    "        index = 88\n",
    "    error_signal = sample - predsample\n",
    "    \n",
    "    # return new ADPCM code code & 0x0f == code\n",
    "    return code & 0x0f\n",
    "\n",
    "\n",
    "\n",
    "def predictor(code,predsample,diffq):\n",
    "    # fixed predictor to get new predicted sample\n",
    "    if code & 8:\n",
    "        predsample = predsample - diffq\n",
    "    else:\n",
    "        predsample = predsample + diffq\n",
    "         \n",
    "    # check for overflow\n",
    "    if predsample > 32767:\n",
    "        predsample = 32767\n",
    "    elif predsample < -32768:\n",
    "        predsample = -32768\n",
    "    #print(\"code:\",code)\n",
    "    #print(\"predsample:\",predsample)\n",
    "    #print(\"diffq:\",diffq)\n",
    "    return predsample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702da379",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADPCM_Decode.\n",
    "# code: a byte containing a 4-bit ADPCM sample.\n",
    "# retval: 16-bit ADPCM sample\n",
    "\n",
    "de_index = 0\n",
    "de_predsample = 0\n",
    "\n",
    "# # This method is the ADPCM Decoder \n",
    "def ADPCM_Decode(code):\n",
    "    global de_index\n",
    "    global de_predsample\n",
    "\n",
    "    step_size = StepSizeTable[de_index]\n",
    "\n",
    "    # inverse code into diff    \n",
    "    diffq = step_size >> 3  # == step/8\n",
    "    if code & 4:\n",
    "        diffq += step_size\n",
    "\n",
    "    if code & 2:\n",
    "        diffq += step_size >> 1\n",
    "\n",
    "    if code & 1:\n",
    "        diffq += step_size >> 2\n",
    "\n",
    "    #print('diffq:' ,diffq)\n",
    "    \n",
    "    # add diff to predicted sample\n",
    "    if code & 8:\n",
    "        diffq = -diffq\n",
    "\n",
    "    de_predsample += diffq\n",
    "\n",
    "    # check for overflow  clip the values to +/- 2^11 (supposed to be 16 bits)\n",
    "    if de_predsample > 32767:\n",
    "        de_predsample = 32767\n",
    "    elif de_predsample < -32768:\n",
    "        de_predsample = -32768\n",
    "\n",
    "    # find new quantizer step size\n",
    "    de_index += IndexTable[code]\n",
    "\n",
    "    # check for overflow\n",
    "    if de_index < 0:\n",
    "        de_index = 0\n",
    "\n",
    "    if de_index > 88:\n",
    "        de_index = 88\n",
    "\n",
    "    # save predict sample and de_index for next iteration\n",
    "    # return new decoded sample\n",
    "    # The original algorithm turned out to be 12bit, need to convert to 16bit\n",
    "    return de_predsample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d3b61e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(X,Y,U,W,V):\n",
    "    for epoch in range(nepoch):\n",
    "        # Step 2.1: Check the loss on training data ............\n",
    "        loss = 0.0\n",
    "\n",
    "        # do a forward pass to get prediction\n",
    "        for i in range(Y.shape[0]):\n",
    "            x, y = X[i], Y[i]                    # get input, output values of each record\n",
    "            prev_s = np.zeros((hidden_dim, 1))   # here, prev-s is the value of0 the previous activation of hidden layer; which is initialized as all zeroes\n",
    "            for t in range(T):\n",
    "                new_input = np.zeros(x.shape)    # we then do a forward pass for every timestep in the sequence\n",
    "                new_input[t] = x[t]              # for this, we define a single input for that timestep\n",
    "                #new_input[t]=ADPCM_Encode(x[t])\n",
    "                mulu = np.dot(U, new_input)\n",
    "                mulw = np.dot(W, prev_s)\n",
    "                add = mulw + mulu\n",
    "                s = sigmoid(add)\n",
    "                mulv = np.dot(V, s)\n",
    "                prev_s = s\n",
    "            #print(mulu.shape)\n",
    "            #calculate error \n",
    "            loss_per_record = (y - mulv)**2 / 2\n",
    "            loss += loss_per_record\n",
    "        loss = loss / float(y.shape[0])\n",
    "\n",
    "        \n",
    "        # Step 2.2: Check the loss on validation data................\n",
    "        val_loss = 0.0\n",
    "        for i in range(Y_val.shape[0]):\n",
    "            x, y = X_val[i], Y_val[i]\n",
    "            prev_s = np.zeros((hidden_dim, 1))\n",
    "            for t in range(T):\n",
    "                new_input = np.zeros(x.shape)\n",
    "                new_input[t] = x[t]\n",
    "                mulu = np.dot(U, new_input)\n",
    "                mulw = np.dot(W, prev_s)\n",
    "                add = mulw + mulu\n",
    "                s = sigmoid(add)\n",
    "                mulv = np.dot(V, s)\n",
    "                prev_s = s\n",
    "\n",
    "            loss_per_record = (y - mulv)**2 / 2\n",
    "            val_loss += loss_per_record\n",
    "        val_loss = val_loss / float(y.shape[0])\n",
    "    \n",
    "        print('Epoch: ', epoch + 1, ', Loss: ', loss, ', Val Loss: ', val_loss)\n",
    "        #print('Epoch: ', epoch + 1, ', Loss: ', loss)\n",
    "\n",
    "        # Step 2.3: Start actual training .......\n",
    "        # Step 2.3.1: Forward Pass\n",
    "        for i in range(Y.shape[0]):\n",
    "            x, y = X[i], Y[i]\n",
    "\n",
    "            layers = []\n",
    "            prev_s = np.zeros((hidden_dim, 1))\n",
    "            dU = np.zeros(U.shape)\n",
    "            dV = np.zeros(V.shape)\n",
    "            dW = np.zeros(W.shape)\n",
    "\n",
    "            dU_t = np.zeros(U.shape)\n",
    "            dV_t = np.zeros(V.shape)\n",
    "            dW_t = np.zeros(W.shape)\n",
    "\n",
    "            dU_i = np.zeros(U.shape)\n",
    "            dW_i = np.zeros(W.shape)\n",
    "\n",
    "            # forward pass\n",
    "            for t in range(T):\n",
    "                new_input = np.zeros(x.shape)\n",
    "                new_input[t] = x[t]\n",
    "                \n",
    "                mulu = np.dot(U, new_input)\n",
    "                mulw = np.dot(W, prev_s)\n",
    "                add = mulw + mulu\n",
    "                s = sigmoid(add)\n",
    "                mulv = np.dot(V, s)\n",
    "                layers.append({'s':s, 'prev_s':prev_s})\n",
    "                prev_s = s\n",
    "\n",
    "\n",
    "            # Step 2.3.2 : Backpropagate Error ...................    \n",
    "            # derivative of pred\n",
    "            dmulv = (mulv - y)\n",
    "\n",
    "\n",
    "            # backward pass\n",
    "            for t in range(T):\n",
    "                dV_t = np.dot(dmulv, np.transpose(layers[t]['s']))\n",
    "                dsv = np.dot(np.transpose(V), dmulv)\n",
    "\n",
    "                ds = dsv\n",
    "                dadd = add * (1 - add) * ds\n",
    "\n",
    "                dmulw = dadd * np.ones_like(mulw)\n",
    "\n",
    "                dprev_s = np.dot(np.transpose(W), dmulw)\n",
    "\n",
    "\n",
    "                for i in range(t-1, max(-1, t-bptt_truncate-1), -1):\n",
    "                    ds = dsv + dprev_s\n",
    "                    dadd = add * (1 - add) * ds\n",
    "\n",
    "                    dmulw = dadd * np.ones_like(mulw)\n",
    "                    dmulu = dadd * np.ones_like(mulu)\n",
    "\n",
    "                    dW_i = np.dot(W, layers[t]['prev_s'])\n",
    "                    dprev_s = np.dot(np.transpose(W), dmulw)\n",
    "\n",
    "                    new_input = np.zeros(x.shape)\n",
    "                    new_input[t] = x[t]\n",
    "                    dU_i = np.dot(U, new_input)\n",
    "                    dx = np.dot(np.transpose(U), dmulu)\n",
    "\n",
    "                    dU_t += dU_i\n",
    "                    dW_t += dW_i\n",
    "\n",
    "                dV += dV_t\n",
    "                dU += dU_t\n",
    "                dW += dW_t\n",
    "\n",
    "\n",
    "                # Step 2.3.3 : Update weights ...................\n",
    "                if dU.max() > max_clip_value:\n",
    "                    dU[dU > max_clip_value] = max_clip_value\n",
    "                if dV.max() > max_clip_value:\n",
    "                    dV[dV > max_clip_value] = max_clip_value\n",
    "                if dW.max() > max_clip_value:\n",
    "                    dW[dW > max_clip_value] = max_clip_value\n",
    "\n",
    "\n",
    "                if dU.min() < min_clip_value:\n",
    "                    dU[dU < min_clip_value] = min_clip_value\n",
    "                if dV.min() < min_clip_value:\n",
    "                    dV[dV < min_clip_value] = min_clip_value\n",
    "                if dW.min() < min_clip_value:\n",
    "                    dW[dW < min_clip_value] = min_clip_value\n",
    "\n",
    "            # update\n",
    "            U -= learning_rate * dU\n",
    "            V -= learning_rate * dV\n",
    "            W -= learning_rate * dW  \n",
    "\n",
    "\n",
    "def model(X,Y,U,W,V):   \n",
    "    preds = []\n",
    "    for i in range(Y.shape[0]):\n",
    "        x, y = X[i], Y[i]\n",
    "        prev_s = np.zeros((hidden_dim, 1))\n",
    "        # Forward pass\n",
    "        for t in range(T):\n",
    "            mulu = np.dot(U, x)\n",
    "            mulw = np.dot(W, prev_s)\n",
    "            add = mulw + mulu\n",
    "            s = sigmoid(add)\n",
    "            mulv = np.dot(V, s)\n",
    "            prev_s = s\n",
    "\n",
    "        preds.append(mulv)\n",
    "\n",
    "    preds = np.array(preds)\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b3ac67",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import IPython.display as ipd\n",
    "import librosa\n",
    "import librosa.display\n",
    "import numpy as np\n",
    "\n",
    "import wave\n",
    "from scipy.io import wavfile\n",
    "\n",
    "np.set_printoptions(threshold=np.inf)\n",
    "audio_file = 'C:/Users/GM/Desktop/audio_data/myspeech.wav'\n",
    "fs, wavdata = wavfile.read(audio_file)\n",
    "wavdata=wavdata[::1]\n",
    "\n",
    "audio, fs = librosa.load(audio_file, sr = 44100)\n",
    "#wavdata=audio[15000:20000]\n",
    "\n",
    "\n",
    "# ===============Step 0: Data Preparation==============\n",
    "X = []\n",
    "Y = []\n",
    "seq_len = 10\n",
    "num_records = len(wavdata) - seq_len\n",
    "\n",
    "for i in range(num_records - 10):\n",
    "    X.append(wavdata[i:i+seq_len]) # the first 50 siquencial samples \n",
    "    Y.append(wavdata[i+seq_len])   # the 51st predicted single sample for the next it used to predictthe next 52nd sample\n",
    "X = np.array(X)\n",
    "X = np.expand_dims(X, axis=2)\n",
    "Y = np.array(Y)\n",
    "Y = np.expand_dims(Y, axis=1)\n",
    "\n",
    "# ===============validation data preparation==============\n",
    "X_val = []\n",
    "Y_val = []\n",
    "for i in range(num_records - 10, num_records):    \n",
    "    X_val.append(wavdata[i:i+seq_len])\n",
    "    Y_val.append(wavdata[i+seq_len])\n",
    "X_val = np.array(X_val)\n",
    "X_val = np.expand_dims(X_val, axis=2)\n",
    "Y_val = np.array(Y_val)\n",
    "Y_val = np.expand_dims(Y_val, axis=1)\n",
    "\n",
    "\n",
    "# =================RNN Architectur========================\n",
    "learning_rate = 0.0001    \n",
    "nepoch = 25             \n",
    "T = 10                   # length of sequence\n",
    "hidden_dim = 100         \n",
    "output_dim = 1\n",
    "\n",
    "bptt_truncate = 5\n",
    "min_clip_value = -10\n",
    "max_clip_value = 10\n",
    "    \n",
    "U = np.random.uniform(0, 1, (hidden_dim, T))\n",
    "W = np.random.uniform(0, 1, (hidden_dim, hidden_dim))\n",
    "V = np.random.uniform(0, 1, (output_dim, hidden_dim))\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "#===============Fucnction Calling train_model==============\n",
    "train_model(X,Y,U,W,V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af6d4fe6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb12547a",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds=model(X,Y,U,W,V)\n",
    "\n",
    "plt.plot(preds[:, 0], 'b')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(Y[:, 0], 'r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b419602",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Y[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a5ab847",
   "metadata": {},
   "outputs": [],
   "source": [
    "#preds[:, 0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
